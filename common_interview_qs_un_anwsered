How is this related to the p-­‐value for the t-­‐sta5s5c in the usual model output? How is this related to the F-­‐sta5s5c in the usual linear regression output? 
How to account for categorical variables? 
What if you want to change the baseline? 
How to account for interac Why?

How do you deal w overfitting?

Does the model make any important assumptions about the data? When might these be 
unrealistic? 
    How do we examine the data to test whether these assumptions are satisfied?

Does the model have convergence problems? Does it have a random component or will the same training data always generate the same model? 
    How do we deal with random effects in training?

What types of data (numerical, categorical etc…) can the model handle?

Can the model handle missing data? What could we do if we find missing fields in our data?

How would explain a neural network to business units?

What alternative models might we use for the same type of problem that this one attempts to solve, and how does it compare to those?

Can we update the model without retraining it from the beginning?
    Some models are trained using Stochastic Gradient Descent and can be updated on a per batch basis. As opposed to gradient descent which needs to iterate through the entire training set again to find the optimal beta coefficients. Sklearn has an SGD classifier with various objective functions that can be trained on a per batch basis. K-means mini-batch is also able to train on a per batch basis which is why it makes for a great online model.

What methods for dimensionality reduction do you know and how do they compare with each other?
    MDS -- ?
    PCA --
    TSNE -- t-distributed Stochastic Neighbor Embedding. It's kind of the opposite of the kernal trick in the context of mapping to dimsional spaces. it maps from a high dimensional space to a lower one 2d or 3d to be specific. It's amazing because it creates the relationships (similarities) in the high dimensional space and then finds a representation for it in the lower coordinate space. Unlike PCA if features have a non-linear relationship it'll retain it. This is specifically to be used for visualization purposes only and as an EDA tool unlike the other methods on here.

    Local Linear Embedding --
    Isomap -- 
    LSH  -- Locality Sensitivity Hashing. I've seen it used in the context of NLP. It leverages the concept of collisions in a hashtable. Basically if two strings are very similar but not quite the same they'll have similar hashes. It'll differ the two hashes (if at all) in the specific area that it differs. Hence it's a great way to limit the search space because it'll create general 'buckets' (hashes) for each data point categorizing all the similar ones together.


How fast is prediction compared to other models? How fast is training compared to other models?


Does the model have any meta-parameters and thus require tuning? How do we do this?

(Deeper machine learning questions)
What is the EM algorithm? Give a couple of applications

What is deep learning and what are some of the main characteristics that distinguish it from traditional machine learning

What is linear in a generalized linear model?

What is a probabilistic graphical model? What is the difference between Markov networks and Bayesian networks?

Give an example of an application of non-negative matrix factorization (NMF)?
    topic modeling

On what type of ensemble technique is a random forest based? What particular limitation does it try to address?
    
What are some good ways for performing feature selection that do not involve exhaustive search?

How would you evaluate the quality of the clusters that are generated by a run of K-means?
(Tools and research)

Do you have any research experience in machine learning or a related field? Do you have any publications?

What tools and environments have you used to train and assess models?

Do you have experience with Spark ML or another platform for building machine learning models using very large datasets?

Discuss your views on the relationship between machine learning and statistics.

Talk about how Deep Learning (or XYZ method) fits (or not?) within the field.

How are kernel methods different?

Why do we need/want the bias term?

Why do we call it GLM when it's clearly non-linear? (somewhat tricky question, to be asked somewhat humorously---but extremely revealing.)

Discuss the meaning of the ROC curve, and write pseudo-code to generate the data for such a curve.

Discuss how you go about feature engineering (look for both intuition and specific evaluation technique)

recommender system

What are the inputs? What are the labels you’re trying to predict? What machine learning algorithms could you run on the data?

How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.

Explain what precision and recall are. How do they relate to the ROC curve?
What is statistical power?

Is it better to have too many false positives, or too many false negatives? Explain.

1. What’s the difference between a decision tree and a decision forest?
2. How would you combat overfitting of a model? 
5. Whats MapReduce and how does it work?
2. Explain a simple map reduce problem
4. Whats more likely: Getting at least one six in 6 rolls, at least two sixes in 12 rolls or at least 100 sixes in 600 rolls?
5. Find all the combinations of change you can for a given amount
3. List several ML techniques. Explain logistic regression and it's loss function. 


What's kernalization?
    https://class.coursera.org/ml-005/lecture/71

Walk me through SVM?
    https://class.coursera.org/ml-005/lecture

What's measure theory?
What's the bias versus variance trade-off?
    https://class.coursera.org/ml-005/lecture/62

How is LDA different from PCA?

How do convolutional networks work?
    https://www.quora.com/How-do-convolutional-neural-networks-work

How would you distinguish between businesses given their contact/address strings if they had a small edit distance?
How would you explain to an engineer how to interpret a p-value?
    it's a statistic that used testing a hypothesis.

What is a kernel? What's a guassian kernel?

What's the density of a Gaussian?

how would you explain to an engineer how to interpret a p-value?

How do you find the optimal variable to drop?
How do you scale up Euclidean distance?
How do you evalaute a model, how do you select a model?
How to penalize one class more than another?
How would you intentionally overfit a model?
When is it better to use LR versus RF?
What’s the best Metric for evaluating a model for a multilabel problem?
Explain regression.


Distributions
-------------
- What's the poisson distribution used to model?
- What's the exponential distribution used to model?
- What's the diff between poisson and exponential?


Sample Variance
----------------------

How do you build a ROC curve?
Does independence imply mutually exclusive?
How could you do an AB test using a classifier?
AIC versus AUC?

Is it prone to over-fitting? If so – what can be done about this?

Does the model make any important assumptions about the data? When might these be 
unrealistic? 
    How do we examine the data to test whether these assumptions are satisfied?

Does the model have convergence problems? Does it have a random component or will the same training data always generate the same model? 
    How do we deal with random effects in training?

What types of data (numerical, categorical etc…) can the model handle?

Can the model handle missing data? What could we do if we find missing fields in our data?

How would explain a neural network to business units?

What alternative models might we use for the same type of problem that this one attempts to solve, and how does it compare to those?

Can we update the model without retraining it from the beginning?

How fast is prediction compared to other models? How fast is training compared to other models?

Does the model have any meta-parameters and thus require tuning? How do we do this?

(Deeper machine learning questions)
What is the EM algorithm? Give a couple of applications

What is deep learning and what are some of the main characteristics that distinguish it from traditional machine learning

What is linear in a generalized linear model?

What is a probabilistic graphical model? What is the difference between Markov networks and Bayesian networks?

Give an example of an application of non-negative matrix factorization

On what type of ensemble technique is a random forest based? What particular limitation does it try to address?

What methods for dimensionality reduction do you know and how do they compare with each other?

What are some good ways for performing feature selection that do not involve exhaustive search?

How would you evaluate the quality of the clusters that are generated by a run of K-means?
(Tools and research)

Do you have any research experience in machine learning or a related field? Do you have any publications?

What tools and environments have you used to train and assess models?

Do you have experience with Spark ML or another platform for building machine learning models using very large datasets?

Discuss your views on the relationship between machine learning and statistics.

Talk about how Deep Learning (or XYZ method) fits (or not?) within the field.

How are kernel methods different?

Why do we need/want the bias term?

Why do we call it GLM when it's clearly non-linear? (somewhat tricky question, to be asked somewhat humorously---but extremely revealing.)

Discuss the meaning of the ROC curve, and write pseudo-code to generate the data for such a curve.

Discuss how you go about feature engineering (look for both intuition and specific evaluation technique)

recommender system

What are the inputs? What are the labels you’re trying to predict? What machine learning algorithms could you run on the data?

How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.

Explain what precision and recall are. How do they relate to the ROC curve?
What is statistical power?

Is it better to have too many false positives, or too many false negatives? Explain.

Is it better to have too many false positives, or too many false negatives? Explain.

What’s the difference between a decision tree and a decision forest?
How would you combat overfitting of a model? 

Whats MapReduce and how does it work?

Explain a simple map reduce problem
How does feature importance work for a random forest?
List several ML techniques. Explain logistic regression and it's loss function. 
What are Recommender Systems?
During analysis, how do you treat missing values?
Reservoir sampling (not only understanding, but also the code and the math behind it)

Stats
-------
Whats more likely: Getting at least one six in 6 rolls, at least two sixes in 12 rolls or at least 100 sixes in 600 rolls?

What is the power anlaysis
What is MLE?
Explain the use of Combinatorics in data science?
What is an Eigenvalue and Eigenvector?
Probability and Statistics Interview Questions for Data Science
There are two companies manufacturing electronic chip. Company A is manufactures defective chips with a probability of 20% and good quality chips with a probability of 80%. Company B manufactures defective chips with a probability of 80% and good chips with a probability of 20%. If you get just one electronic chip, what is the probability that it is a good chip?
Suppose that you now get a pack of 2 electronic chips coming from the same company either A or B. When you test the first electronic chip it appears to be good. What is the probability that the second electronic chip you received is also good?
A coin is tossed 10 times and the results are 2 tails and 8 heads. How will you analyse whether the coin is fair or not? What is the p-value for the same?
What are the assumptions required for linear regression?

Confidence intervals
How they are constructed
Why you standardize
How to interpret

Sampling
Why and when?
How do you calculate needed sample size? [Power analysis is advanced]
Limitations
Bootstrapping and resampling?
Biases
When you sample, what bias are you inflicting?
How do you control for biases?
What are some of the first things that come to mind when I do X in terms of biasing your data?
Experimentation 
How do test new concepts or hypotheses in....insert domain X? i.e. How would evaluate whether or not consumers like the webpage redesign or new food being served?
How do you create test and control groups?
How do you control for external factors?
How do you evaluate results?
Whats more likely: Getting at least one six in 6 rolls, at least two sixes in 12 rolls or at least 100 sixes in 600 rolls? 

If the linear correlation between two random variables is negative and negative what does that mean but when you measure the coefficients in conjunction with eachother then it’s positive?


1. You have three coins in your pocket. Two fair coins and one two-headed coin. You pick out a coin and flip heads twice. What is the probability that you picked the two-headed coin?

1. Your 10-person team decides to meet at Salumeria failing to recall that there are two Salumerias. Assuming that everyone has a 50-50 chance of choosing each Salumeria, what is the probability that everyone ends up at the same place?

1. It rains 10% of days. It's sunny 75% of days. It's rainy & sunny 1% of the days. What percent of the days is it neither sunny nor rainy?

1. We've already run an A/B test and these are the results:

    ```
    GROUP A: 1000 views, 100 buys
    GROUP B: 1000 views, 80 buys
    ```

    Can we say with confidence that the new version is better/worse?

-----

1. You are trying to fit a polynomial to your data. How will you determine what degrees of polynomial are overfitting/underfitting/just right? Draw what you would expect the graph of train and test error to look like.

1. What are the random aspects of a random forest?

1. Why do statsmodels/sklearn use the normal equation rather than gradient descent for solving linear regression?

1. How do you do non-binary classification with logistic regression?
1. What's the difference between how decision tree classifiers and decision tree regressors work?




Questions
----------
how would you do A/B testing with supervised learning?
Model deployment best practices?
What are some noise removal and anomaly detection techniques?
What are use cases for PCA, in areas which it's superior to other algorithms?
How do you gauge cluster quality with DBSCAN?
Which models are more stable: parametric or non-parametric?
LDA versus PCA? and it's optimal use-cases?
What is pca's inverse transform method?
What is the optimal method to choose components for PCA?
    what are the trade-offs for elbow method, kaiser, grid-searching
How would you know the number of principal components to choose for getting rid of the anomalies?
What are some methods for anomaly detection in time-series data? And how do they work?
What does Series.select_dtypes do?
What does Series.value_counts(normalize=true) do?
What is the gale-shapley algo?
What is the PCA feature contribution method?
How do you emphasize a particular feature more or less

http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/
    Why is logistic regression a regression model?
    Distinguish between regression learning, tree-based, instance-based, bayesian? 
    What does recommender systems fall into?




Kuhn: remedies for severe class imbalance? and data mining?


Model evaluation
=================
- What’s the best Metric for evaluating a model for a multilabel problem?
- What are best practices when deciding if one model is better than another?
- How are you prototyping? test/train validation.
- Do you use a seperate data set when you're training for feature engineering and grid-searching hyper-parameters, to model evaluation?
- When is it better to use LR versus RF?
- How do you deal with class imbalance?



Deduplication techniques
=======================
- What are some popular ones you know of?
- One's that aren't quadratic and expensive in terms of time-complexity?
- Graph theory

Log_loss
========
How does log_loss work?

Anomaly detection
==================
Anomaly detection techniques?



Does the model make any important assumptions about the data? When might these be 
unrealistic? 
What are some dimensionality reduction algorithms you've used in practice besides PCA?
Are there any situations where you'd want the model to be overfit?
What's a GLM?
How would you combat overfitting of a model?
What's the difference between a regular MR job and streaming MR job?
How to penalize one class more than another?



*Take a look at Sebastians blog


Multicollinearity
==============
What are consquences for Multicollinearity?
    - an unstable model
    - beta coefficients are incorrect and could be non-sensical. eg lets the beta coefficients are all positive and all of the sudden some go negative as soon as you add a feature.. this is a sign of multicollinearity.
What are indications of multicollinearity?
What solutions to multicollinearity?




Best,
===========

- feature selection?
    supervised models
    missing value ratio per column: basically remove columns missing information

    unsupervised models?

- active learning?
    There are methods of updating a model via highest information gain, static s.t. thier periodic to avoid any type of biases, 
- what’s more robust to outliers l1 or l2?
    this is a non-sensical question as this pertains to penalizing the betaweights of a feature as a opposed to a particular example

- what's the hotsax approach?
- all dim reduction methods?
- hierarchical clustering versus k-means?
- what's better for outliers SVM or Logistic regression?
- what are situations that don't require scaling to the 0 mean unit variance?
- What's the distinction between covariance and correlation?
    Correlation is a normalized s.t. that everything's on the same scale i.e. -1 to 1. It provides for a better basis of comparison. This is achieved by normalization i.e. dividing variance or covariance by std deviation.
    Covariance is the measure of how these to variables move in proportion to one another and doesn't take units of measurement into account.
- why does PCA require scaling?
    Because it requires figuring out the correlation or covariance of a matrix which takes into account the pairwise differences of features/examples which requires them to be on the same scale.

- How to handle the loss of meaning after doing PCA?
    - Wrapper models: Roc auc benchmark but it would tell you direction so what you can do is the method where you take a contrived set of examples and just move variables around and plot the probability as you change it. This isn't an exact science but I think it's one of those good enough cures.

    - inverse-transform: suppose to you do the transformation to prune away anomalies or something th
    - variable loadings in each fo the components

- Should you not scale if you're using NMF?
    Both are matrix decomposition techniques. Original matrix in which we are interested may be very 'big', sparse, with no order. Factoring it would yield a set of more manageable, compact and ordered matrices. Further, using these factors, it is easy to find hidden relationships (if any) like correlation, orthogonality, sub-space/ span/ projection in the original matrix. Many decomposition exists for a matrix viz. Singular value decomposition (SVD), Non-negative matrix factorization (NMF or NNMF), LU decomposition, QR decomposition, Cholskey decomposition etc. The difference comes from the applicability and physical significance that can be deduced by these based on domain in which they are applied (e.g. Signal processing with applications to wireless communication, time-series analysis, speech and image processing, data compression and storage, Text and Data mining, Privacy protection/ data encoding etc.). Here are few pointers:

    1. SVD factors contains both positive and negative entries while NMF factors are strictly positive. This is useful in case of text mining where usually TF or TF-IDF matrix is being factorized. Since entries of original matrix are positive and has physical meaning (term frequencies...), once would want factors to be positive so that physical connections are be directly made.
    2. SVD factors can be related to Eigen-functions of a system when original matrix represented a system about which one is interested in terms of signal processing perspective. They are used in sub-space decomposition, characteristic matrices (useful in speech and image processing - face recognition etc.). NMF can also be used but the associating physical relationships are more indirect and hence 'tedious'/ 'more mathematically involved'
    3. SVD yields unique factors whereas NMF factors are non-unique. This makes NMF more suitable for privacy protection algorithms.

- How does RF feature importance work?
- How you design a unit test for overfitt and underfitt?
- what's the hotsax approach?
- what’s more robust to outliers l1 or l2
- why is document fingerprinting better with cosine similarity versus euclidian distance
- what do you want to do? (when asked by a big company)
- what do you want to do? (when asked by a startup)
- how do you grid-search the penalty parameter for elasticnet?
    - You generally have to figure out the ratio of L1 vs L2 which is some number [0,1] selection cycle = 'cyclic'. alpha which is actually the weight on the penalty parameter where 0 is the equiv of OLS i.e. no regularization



What's a link function?
We could us link functions to relate linear regression to 

log(p/1-p)=Ax+B => p=(1/(1+e^-(Ax + B)))

p=prob(y=1|x_i, bib0)
the probability that someone survive (1) given the features you see

y in linear regression is thougt to be normally distributed

What is multinomial logistic regression

How is the sigmoid function related to the logit function?
it's the inverse of one another?

What is heteroskadiscity?
How would you summarize normal approximation and heteroskadiscity?

Ax + B stretchs or compresses the sigmoid

logit function takes in odds and converts to log odds and sigmoid inverts the function.

What could I have done better with take-home

What's the cost function for logistic/linear regression

Business reason to drop category that have some ordinality because it's much more interpreable. So take care in dropping it.

WHat's it mean when a loss function is convexed?

Walk me through how to fit Logistic Regression Step-by-step?

WHen do you use classifier with a regression problem

Do Decision trees interact the features for you?

deciles charts

#what's the difference between MB and Naive Bayes?
#Which similarity metric is the best for which situations:
    # pearson
    # jaccard
    # manhattan
    # cosine_similarity

#what's the difference between MB and Naive Bayes?

#what's the difference between adjusted R**2
#what's the difference between MB and Naive Bayes?

What is BIC?

it's a meric used to find the best fit model with varying complexities. It also shows at which point a model might be overfit. Meaning the incremental change in fit due to model complexity might be harmful. It penalizes the number of parameters for a model to give you an acurrate view of fit gain by adding additional parameters. As tool it aid in providing a good fit while avoiding overfitting.

P(data | model) / P(data | all_models)

What is the gap statistic?

What is the elbow method?

What is Silouette?

With k-means or any other clustering metric this coefficient indicates how well a points belongs in it's respective cluster. 

+1: the point it far away from a neighboring cluster
0: the point is on the boundary
-: the point should've been assigned to another cluster

Calculate the coefficient for every single cluster and plot it cluster by cluster. If any of the points are in the negative then it doesn't belong in that cluster. If the distribution is uniform, it does belong in that cluster.