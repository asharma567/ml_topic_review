How is this related to the p-­‐value for the t-­‐sta5s5c in the usual model output? How is this related to the F-­‐sta5s5c in the usual linear regression output? 
How to account for categorical variables? 
What if you want to change the baseline? 
How to account for interac Why?

How do you deal w overfitting?

Does the model make any important assumptions about the data? When might these be 
unrealistic? 
    How do we examine the data to test whether these assumptions are satisfied?

Does the model have convergence problems? Does it have a random component or will the same training data always generate the same model? 
    How do we deal with random effects in training?

What types of data (numerical, categorical etc…) can the model handle?

Can the model handle missing data? What could we do if we find missing fields in our data?

How would explain a neural network to business units?

What alternative models might we use for the same type of problem that this one attempts to solve, and how does it compare to those?

Can we update the model without retraining it from the beginning?
    Some models are trained using Stochastic Gradient Descent and can be updated on a per batch basis. As opposed to gradient descent which needs to iterate through the entire training set again to find the optimal beta coefficients. Sklearn has an SGD classifier with various objective functions that can be trained on a per batch basis. K-means mini-batch is also able to train on a per batch basis which is why it makes for a great online model.

What methods for dimensionality reduction do you know and how do they compare with each other?
    MDS -- ?
    PCA --
    TSNE -- t-distributed Stochastic Neighbor Embedding. It's kind of the opposite of the kernal trick in the context of mapping to dimsional spaces. it maps from a high dimensional space to a lower one 2d or 3d to be specific. It's amazing because it creates the relationships (similarities) in the high dimensional space and then finds a representation for it in the lower coordinate space. Unlike PCA if features have a non-linear relationship it'll retain it. This is specifically to be used for visualization purposes only and as an EDA tool unlike the other methods on here.

    Local Linear Embedding --
    Isomap -- 
    LSH  -- Locality Sensitivity Hashing. I've seen it used in the context of NLP. It leverages the concept of collisions in a hashtable. Basically if two strings are very similar but not quite the same they'll have similar hashes. It'll differ the two hashes (if at all) in the specific area that it differs. Hence it's a great way to limit the search space because it'll create general 'buckets' (hashes) for each data point categorizing all the similar ones together.


How fast is prediction compared to other models? How fast is training compared to other models?


Does the model have any meta-parameters and thus require tuning? How do we do this?

(Deeper machine learning questions)
What is the EM algorithm? Give a couple of applications

What is deep learning and what are some of the main characteristics that distinguish it from traditional machine learning

What is linear in a generalized linear model?

What is a probabilistic graphical model? What is the difference between Markov networks and Bayesian networks?

Give an example of an application of non-negative matrix factorization (NMF)?
    topic modeling

On what type of ensemble technique is a random forest based? What particular limitation does it try to address?
    
What are some good ways for performing feature selection that do not involve exhaustive search?

How would you evaluate the quality of the clusters that are generated by a run of K-means?
(Tools and research)

Do you have any research experience in machine learning or a related field? Do you have any publications?

What tools and environments have you used to train and assess models?

Do you have experience with Spark ML or another platform for building machine learning models using very large datasets?

Discuss your views on the relationship between machine learning and statistics.

Talk about how Deep Learning (or XYZ method) fits (or not?) within the field.

How are kernel methods different?

Why do we need/want the bias term?

Why do we call it GLM when it's clearly non-linear? (somewhat tricky question, to be asked somewhat humorously---but extremely revealing.)

Discuss the meaning of the ROC curve, and write pseudo-code to generate the data for such a curve.

Discuss how you go about feature engineering (look for both intuition and specific evaluation technique)

recommender system

What are the inputs? What are the labels you’re trying to predict? What machine learning algorithms could you run on the data?

How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.

Explain what precision and recall are. How do they relate to the ROC curve?
What is statistical power?

Is it better to have too many false positives, or too many false negatives? Explain.

1. What’s the difference between a decision tree and a decision forest?
2. How would you combat overfitting of a model? 
5. Whats MapReduce and how does it work?
2. Explain a simple map reduce problem
4. Whats more likely: Getting at least one six in 6 rolls, at least two sixes in 12 rolls or at least 100 sixes in 600 rolls?
5. Find all the combinations of change you can for a given amount
3. List several ML techniques. Explain logistic regression and it's loss function. 


What's kernalization?
    https://class.coursera.org/ml-005/lecture/71

Walk me through SVM?
    https://class.coursera.org/ml-005/lecture

What's measure theory?
What's the bias versus variance trade-off?
    https://class.coursera.org/ml-005/lecture/62

How is LDA different from PCA?

How do convolutional networks work?
    https://www.quora.com/How-do-convolutional-neural-networks-work

How would you distinguish between businesses given their contact/address strings if they had a small edit distance?
How would you explain to an engineer how to interpret a p-value?
    it's a statistic that used testing a hypothesis.

What is a kernel? What's a guassian kernel?

What's the density of a Gaussian?

how would you explain to an engineer how to interpret a p-value?

How do you find the optimal variable to drop?
How do you scale up Euclidean distance?
How do you evalaute a model, how do you select a model?
How to penalize one class more than another?
How would you intentionally overfit a model?
When is it better to use LR versus RF?
What’s the best Metric for evaluating a model for a multilabel problem?
Explain regression.


Distributions
-------------
- What's the poisson distribution used to model?
- What's the exponential distribution used to model?
- What's the diff between poisson and exponential?


Sample Variance
----------------------

How do you build a ROC curve?
Does independence imply mutually exclusive?
How could you do an AB test using a classifier?
AIC versus AUC?

Is it prone to over-fitting? If so – what can be done about this?

Does the model make any important assumptions about the data? When might these be 
unrealistic? 
    How do we examine the data to test whether these assumptions are satisfied?

Does the model have convergence problems? Does it have a random component or will the same training data always generate the same model? 
    How do we deal with random effects in training?

What types of data (numerical, categorical etc…) can the model handle?

Can the model handle missing data? What could we do if we find missing fields in our data?

How would explain a neural network to business units?

What alternative models might we use for the same type of problem that this one attempts to solve, and how does it compare to those?

Can we update the model without retraining it from the beginning?

How fast is prediction compared to other models? How fast is training compared to other models?

Does the model have any meta-parameters and thus require tuning? How do we do this?

(Deeper machine learning questions)
What is the EM algorithm? Give a couple of applications

What is deep learning and what are some of the main characteristics that distinguish it from traditional machine learning

What is linear in a generalized linear model?

What is a probabilistic graphical model? What is the difference between Markov networks and Bayesian networks?

Give an example of an application of non-negative matrix factorization

On what type of ensemble technique is a random forest based? What particular limitation does it try to address?

What methods for dimensionality reduction do you know and how do they compare with each other?

What are some good ways for performing feature selection that do not involve exhaustive search?

How would you evaluate the quality of the clusters that are generated by a run of K-means?
(Tools and research)

Do you have any research experience in machine learning or a related field? Do you have any publications?

What tools and environments have you used to train and assess models?

Do you have experience with Spark ML or another platform for building machine learning models using very large datasets?

Discuss your views on the relationship between machine learning and statistics.

Talk about how Deep Learning (or XYZ method) fits (or not?) within the field.

How are kernel methods different?

Why do we need/want the bias term?

Why do we call it GLM when it's clearly non-linear? (somewhat tricky question, to be asked somewhat humorously---but extremely revealing.)

Discuss the meaning of the ROC curve, and write pseudo-code to generate the data for such a curve.

Discuss how you go about feature engineering (look for both intuition and specific evaluation technique)

recommender system

What are the inputs? What are the labels you’re trying to predict? What machine learning algorithms could you run on the data?

How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.

Explain what precision and recall are. How do they relate to the ROC curve?
What is statistical power?

Is it better to have too many false positives, or too many false negatives? Explain.

Is it better to have too many false positives, or too many false negatives? Explain.

What’s the difference between a decision tree and a decision forest?
How would you combat overfitting of a model? 

Whats MapReduce and how does it work?

Explain a simple map reduce problem
How does feature importance work for a random forest?
List several ML techniques. Explain logistic regression and it's loss function. 
What are Recommender Systems?
During analysis, how do you treat missing values?
Reservoir sampling (not only understanding, but also the code and the math behind it)

Stats
-------
Whats more likely: Getting at least one six in 6 rolls, at least two sixes in 12 rolls or at least 100 sixes in 600 rolls?

What is the power anlaysis
What is MLE?
Explain the use of Combinatorics in data science?
What is an Eigenvalue and Eigenvector?
Probability and Statistics Interview Questions for Data Science
There are two companies manufacturing electronic chip. Company A is manufactures defective chips with a probability of 20% and good quality chips with a probability of 80%. Company B manufactures defective chips with a probability of 80% and good chips with a probability of 20%. If you get just one electronic chip, what is the probability that it is a good chip?
Suppose that you now get a pack of 2 electronic chips coming from the same company either A or B. When you test the first electronic chip it appears to be good. What is the probability that the second electronic chip you received is also good?
A coin is tossed 10 times and the results are 2 tails and 8 heads. How will you analyse whether the coin is fair or not? What is the p-value for the same?
What are the assumptions required for linear regression?

Confidence intervals
How they are constructed
Why you standardize
How to interpret

Sampling
Why and when?
How do you calculate needed sample size? [Power analysis is advanced]
Limitations
Bootstrapping and resampling?
Biases
When you sample, what bias are you inflicting?
How do you control for biases?
What are some of the first things that come to mind when I do X in terms of biasing your data?
Experimentation 
How do test new concepts or hypotheses in....insert domain X? i.e. How would evaluate whether or not consumers like the webpage redesign or new food being served?
How do you create test and control groups?
How do you control for external factors?
How do you evaluate results?
Whats more likely: Getting at least one six in 6 rolls, at least two sixes in 12 rolls or at least 100 sixes in 600 rolls? 

If the linear correlation between two random variables is negative and negative what does that mean but when you measure the coefficients in conjunction with eachother then it’s positive?

Multicollinearity
==============
What are consquences for Multicollinearity?
What are indications of multicollinearity?
What solutions to multicollinearity?


1. You have three coins in your pocket. Two fair coins and one two-headed coin. You pick out a coin and flip heads twice. What is the probability that you picked the two-headed coin?

1. Your 10-person team decides to meet at Salumeria failing to recall that there are two Salumerias. Assuming that everyone has a 50-50 chance of choosing each Salumeria, what is the probability that everyone ends up at the same place?

1. It rains 10% of days. It's sunny 75% of days. It's rainy & sunny 1% of the days. What percent of the days is it neither sunny nor rainy?

1. We've already run an A/B test and these are the results:

    ```
    GROUP A: 1000 views, 100 buys
    GROUP B: 1000 views, 80 buys
    ```

    Can we say with confidence that the new version is better/worse?

-----

1. You are trying to fit a polynomial to your data. How will you determine what degrees of polynomial are overfitting/underfitting/just right? Draw what you would expect the graph of train and test error to look like.

1. What are the random aspects of a random forest?

1. Why do statsmodels/sklearn use the normal equation rather than gradient descent for solving linear regression?

1. How do you do non-binary classification with logistic regression?
1. What's the difference between how decision tree classifiers and decision tree regressors work?




Questions
----------
how would you do A/B testing with supervised learning?
Model deployment best practices?
What are some noise removal and anomaly detection techniques?
What are use cases for PCA, in areas which it's superior to other algorithms?
How do you gauge cluster quality with DBSCAN?
Which models are more stable: parametric or non-parametric?
LDA versus PCA? and it's optimal use-cases?
What is pca's inverse transform method?
What is the optimal method to choose components for PCA?
    what are the trade-offs for elbow method, kaiser, grid-searching
How would you know the number of principal components to choose for getting rid of the anomalies?
What are some methods for anomaly detection in time-series data? And how do they work?
What does Series.select_dtypes do?
What does Series.value_counts(normalize=true) do?
What is the gale-shapley algo?
What is the PCA feature contribution method?
How do you emphasize a particular feature more or less

http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/
    Why is logistic regression a regression model?
    Distinguish between regression learning, tree-based, instance-based, bayesian? 
    What does recommender systems fall into?




Kuhn: remedies for severe class imbalance? and data mining?


Model evaluation
=================
- What’s the best Metric for evaluating a model for a multilabel problem?
- What are best practices when deciding if one model is better than another?
- How are you prototyping? test/train validation.
- Do you use a seperate data set when you're training for feature engineering and grid-searching hyper-parameters, to model evaluation?
- When is it better to use LR versus RF?
- How do you deal with class imbalance?



Deduplication techniques
=======================
- What are some popular ones you know of?
- One's that aren't quadratic and expensive in terms of time-complexity?
- Graph theory

Log_loss
========
How does log_loss work?

Anomaly detection
==================
Anomaly detection techniques?


Does the model make any important assumptions about the data? When might these be 
unrealistic? 
What are some dimensionality reduction algorithms you've used in practice besides PCA?
Are there any situations where you'd want the model to be overfit?
What's a GLM?
How would you combat overfitting of a model?
What's the difference between a regular MR job and streaming MR job?
How to penalize one class more than another?



*Take a look at Sebastians blog


Multicollinearity
==============
What are consquences for Multicollinearity?
What are indications of multicollinearity?
What solutions to multicollinearity?


Seb's q
-------
In any case, I struggled to understand the last section of the post when you were talking about building a Confidence Interval with the generalization error.

I’d love to clarify the sections that are unclear; however, it would be nice if you could give me a further hint regarding the part that was most unclear so that I could tackle these more specifically.
Basically, I was just applying the “normal approximation,” since the predictions are binary; we have a 0-1 loss and the prediction for a given sample is either “right” or “wrong”. Now, if we average these predictions, we get the prediction “accuracy”. For example, let’s assume we have 5 samples. The true class labels are [1 1 0 0 0], and we make a prediction [0 0 0 0 0]. In this case, we have the following 0-1 Loss

0 = 1 -> False
0 = 1 -> False
0 = 0 -> True
0 = 0 -> True
0 = 0 -> True

which gives us a count of 3 correct predictions (think of a binomial distribution and computing the probabiliity of getting at least 3 out of 5 predictions right). The accuracy is then basically just this average number 3/5.

Sorry if this sounds a bit abstract, but maybe google for the term “normal approximation” and browse through some of the articles; there are pretty good articles on this out there.

Best,