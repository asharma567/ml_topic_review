Todos:
    test run sparkcontext on any script DONE

    rewrite-preprocessing in Spark
        use emr script add all packages here: bootstrap_actions.sh


    enable debugging on cluster provision through boto3 
    change spark env variables to python3
        #get link
    enable debug trigger
        #this only gives logs to the steps
    enabling the UI debug bit
    Run the job through SSH first then through uI
    look-up flags for spark submit
    hit-up support for chalice later, explicitly talk about the use-case
    change tiers for computations
	https://aws.amazon.com/ec2/instance-types/
        master_instance_type: m3.xlarge
        slave_instance_type: m3.xlarge



Notes:
    boostrap actions install first before the AWS provisions the clusters

    ssh update EC2 permission with my ip | EC2 console | select security groups | ElasticMapReduce-master
    | inbounds | SSH, MyIP
    append py_stringmatching to pyspark_quick_setup.sh DONE

    running any shell script as a step:
    s3://us-west-2.elasticmapreduce/libs/script-runner/script-runner.jar


Resources:
    boto
    https://medium.com/@datitran/quickstart-pyspark-with-anaconda-on-aws-660252b88c9a

    https://www.themarketingtechnologist.co/upload-your-local-spark-script-to-an-aws-emr-cluster-using-a-simply-python-script/

    https://aws.amazon.com/blogs/big-data/submitting-user-applications-with-spark-submit/

    installing packages via bootstrap for EMR
    http://www.cloudypoint.com/Tutorials/discussion/python-how-to-bootstrap-installation-of-python-modules-on-amazon-emr/

    tutorial on Spark
    https://www.quora.com/What-is-a-good-book-tutorial-to-learn-about-PySpark-and-Spark
    https://spark.apache.org/docs/latest/ml-features.html
    http://spark.apache.org/docs/latest/sql-programming-guide.html
    https://github.com/apache/spark/blob/master/examples/src/main/python/sql/basic.py


http://stackoverflow.com/questions/31525012/how-to-bootstrap-installation-of-python-modules-on-amazon-emr


Logs, AWS loft notes
=====
sudo bash

ctrl+d
cd /mnt/var/log/hadoop-yarn/containers/some_log

spark-submit --deploy-mode cluster preprocessor_emr_stub.py labeled_dataset_2016_10_01_2017_03_21_v2.psv feature_M_spark.csv --py-files preprocessing_v3.py

from preprocessing_v3 import preprocessing_prototyping, RAW_DATA_FIELDS_USED_FOR_MODEL

sc.addPyFile('preprocessing_v3.py')


File "preprocessor_emr_stub.py", line 4, in <module>
    import pandas as pd
ImportError: No module named pandas

spark-submit --deploy-mode cluster s3://rocketrip-preprocessing/preprocessor_emr_stub.py labeled_dataset_2016_10_01_2017_03_21_v2.psv feature_M_spark.csv 

hadoop-streaming -files s3://rocketrip-preprocessing/mapper.py -mapper mapper.py -input s3://rocketrip-preprocessing/labeled_dataset_2016_10_01_2017_03_21_v2.psv -output s3://rocketrip-delete/streaming-out5/

aws s3 cp  s3://rocketrip-preprocessing/ . --recursive
aws s3 cp  s3://rocketrip-preprocessing/nltk_data/ . --recursive
aws s3 mv s3://rocketrip-preprocessing/chunkers s3://rocketrip-preprocessing/nltk_data
aws s3 mv s3://rocketrip-preprocessing/chunkers s3://rocketrip-preprocessing/nltk_data --recursive
aws s3 cp  s3://rocketrip-preprocessing/nltk_data/ nltk_data/ --recursive
aws s3 cp preprocessor_emr_stub.py s3://rocktrip-preprocessing/preprocessor_emr_stub.py
aws s3 cp preprocessor_emr_stub.py s3://rocketrip-preprocessing/preprocessor_emr_stub.py

spark-submit --deploy-mode cluster preprocessor_emr_stub.py labeled_dataset_2016_10_01_2017_03_21_v2.psv feature_M_spark.csv --py-files preprocessing_v3.py