I think we could all agree that EDA is a crucial part of Data Science. Yet I find some of the tools we use rather limiting e.g. scatterplots, <other examples>, etc. I've finally come across something that's extremely useful in visualizing your data - TSNE (t-Distributed Stochastic Neighbor Embedding). It's a very easy to use dimensionality reduction technique specifically for the purpose of visualizaton. 

Before you go reading on to the technical details of the algorithm lets examine of its usecases to see if it could be of help to you.

What are the use-cases?
    - Finding the number of groups within your data set
    - Anomaly detection
    - Checking whether your test and training set is biased 
#link
#plot test/val
#explanation
    - The quality of your training set
#link
#0 and 1 plot i.e. do they overlapp
    - Visualizing the manifold of your data set i.e. the decision boundary. How separable is your data? is it linear or non-linear?
#link
#plot of iris data set with PCA and TSNE
    - Visualizing photos at every stage (layer) of a convolutional neural net i.e. it's a fantastic way to debug your net to see if it's picking up the right features and learning as you intended.
#link
#plots
#preinput
#of data after conv1
#FC
    - Multiple semantic structures your dataset implies. E.g. Tie example (#Link).
    - Finding the similarities/relationships of a user-base. 
#walk through how to do this
    - Ideal for any 2-d and 3-d representations of a feature matrix.
#plot of a 3-d



How does TSNE work?
    The high-level steps are as follows--

    1) You loop through a data set where you're making a pairwise comparison of every single example(observation) against each-other. This process is called finding the affinities (similarities) between the data points. The basic idea is to compute the euclidean distances normalized by a dynamic variance between each point as conditional probabilities: "the probability x_i would pick x_j as it's neighbor."
#the end result being? some distribution

    2) Map it on to a lower dimensional space (often 2-d or 3-d for visualization purposes) then minimize the difference between the representation on the high dimensional space vs the low dimensional space -- cost function is called the Kullback-Leibler.

#Jpeg of the cost function

        * the algorithm is more sensitive to smaller differences between two points rather than larger ones. PCA, on the other-hand is great at capturing the large differences but not the small. Upon inspecting the end result displayed; PCA is good at differentiating the groups (the global structure) but not so good at local structure (intra-groups).

#example either Mnist or houses
#global structure
#zoom in of local structure

    3) Plot it using matplotlib or ggplot. There's an option to show it as points on a plot using standard symbols e.g. dots, triangles, etc. or  superimposing actual thumbnail images the data points correspond to.

#example showing labeled plot, unlabeled plot, super-imposed words.

What are the PROs and CONs of TSNE
    Pros:  
        1) Retains global and local structure of the examples. Meaning how certain clusters can relate to one another and even within the clusters how different points can relate to one another. 
        For example, imagine you were organizing your clothes: First step organizing all your t-shirts together then your jeans, then your long sleeve etc. then going a step further and rank-ordering by color (or some other feature) inherent to the subject clothing article.

        2) It's a superior approach to that of Locally Linear Embedding, Sammon Mapping, Isomap, PCA, etc. It captures more complex non-linear relationships of the examples/observations much better than the other methods. It captures small more intricate distances. Bottomline: it separates the data very well for visual inspection.
#could you show a plot of K-Means?

        3) It could also have multiple maps which capture a word/token's various semantic representations within a corpus of text documents. 
        For example, consider the word 'Tie': In one map it could be very closely associated with the words like 'Tuxedo'/'Suit' and have a low weighted association with the word 'Knot'. In another map it could be the exact opposite which successfully captures both ways 'Tie' could interpreted.

    Cons:
        1) It's strictly made as an EDA tool. It hasn't been proven to work as a sturdy dimensionality reduction tool which is later fed into a prediction model. e.g. PCA, TruncatedSVD.

        2) It doesn't have a predict method nor does it label each point into a cluster. It ingests the data in whole and then displays it. There isn't a way to classify a new/unseen data point.
        A viable solution would be to count the number of groups (k) using TSNE and visual inspection then use K-means to reap the benefits of an online algorithm (i.e. labeling, classifying new data points, etc).

#plot of any feature matrix using TSNE
#feed the number of clusters into K
#but is this any different from DBSCAN?

        3) It's certainly subject to the curse of dimensionality as the original algorithm scales O(N^2) though there's an implementation called Barnes Hut approximations which scales as O(N log N) but a bit less accurate. It's the fastest implementation to date and it makes it easily usable in a reasonable amount of time.

        4) It's not guaranteed to converge to the global optimum and it produces slightly different results every run since it uses Stochastic Gradient Descent.

#What is Barnes-Hut approximations?


Are there are any hyper-parameters used for tuning?
    - P: perplexity. Think of it as 'k' in k-nearest neighbors or min_samples needed for a datapoint to be considered a 'core-point' in DBSCAN.

Can I feed it a pre-computed similarity matrix with using a similarity metric of my choice?
    - Yes! Just like in DB-Scan you can use home-grown or another out-of-the-box similarity metric (other than Euclidean distance). *check this



The quality of your training set
    <front of houses>
    in the plot above we can clearly see that one class clearly falls on one side of the dataset and rest fall on the other, which is indicative of separable classes. But we can see there are a few points which overlap directly on to the other class, these points have a high similarity. 

    If you notice the red points lost among the blue circled. Just by visual inspection, I would say a classifier would missclassify these points.

    #show some results of a classifier scoring these two points

    Solution) upon retraining your classifier one might suggest these points have a very high information gain hence we should put more points like this in the set.

    <ploting the iris set>
    Here's a plot of a dataset whose classes are much more seperable as you can see there are three clearly clusters of points