http://datascience.stackexchange.com/questions/6547/open-source-anomaly-detection-in-python

http://scikit-learn.org/stable/modules/outlier_detection.html

http://scikit-learn.org/stable/auto_examples/applications/plot_outlier_detection_housing.html

https://indico.lal.in2p3.fr/event/2987/session/9/contribution/27/material/slides/0.pdf

http://www.datasciencecentral.com/profiles/blogs/introduction-to-outlier-detection-methods

====
While statistical methods are mathematically
more precise, they suffer from several shortcomings, such as simplified assumptions
about data representations, poor algorithmic scalability, and
a low focus on interpretability. 

With the increasing advances in hardware technology for data collection, and advances in software technology (databases) for data organization, computer scientists have increasingly been participating in the latest advancements of this field. Computer
scientists approach this field based on their practical experiences in managing
large amounts of data, and with far fewer assumptions– the data
can be of any type, structured or unstructured, and may be extremely
large.
====

“Never take the comment that you are different as a condemnation,
it might be a compliment. It might mean that you possess unique
qualities that, like the most rarest of diamonds is ... one of a
kind.” – Eugene Nathaniel Butler

Intrusion Detection Systems
Credit Card Fraud
Interesting Sensor Events
Medical Diagnosis
Law Enforcement
Earth Science

read this introduction: talks about use cases

Page 7) the importance of pickign the proper model given the distribution of the data

read chapter 3: basic models for outlier detection

page 23) Categorical, Text, and mixed attributes

page 24) Time-series data and data streams

page 28) Supervised Outlier Detection





NOTES
----
What is an outlier?
	Is an observation that deviates so much from the norm of the data such which causes sense of suspicion as to how it was generated.

Intrusion Detection System:
	operating system calls, network traffic, other abnormal activity within the system
	- Think about someone hacking into a secured system.

Credit Card Fraud:
	an example of an anomalous use case would be to detect a buying spree from an anomalous geo location

Interesting sensor events:
	sudden change from the norm my mark an interesting event that's occurred which a sensor is tracking.

Law Enforcement: 
	Fraud in credit card transactions, trading activity and other activities by financial institutions. 

Sometime a sequence of points or set of points delineate an anomalous situation.

outlier could be referred to as abnormality or noise, where as anomaly is a special case which is of interest to an analyst.

distinguishing between anomolous behavior and noise can be difficult.
	solution would be to create noise detector. thresholding the outlierness of a point we could classify one as being a weak or a strong outlier. 
	
	Do we have previous examples of outliers?
		To distinguish the two points were could make an inference based on previously examples of anomalies. 

	Supervised methods could be used for noise removal or identifying the anomaly

What is a Gaussian mixture model?
	It's a generative model i.e it generates probabilities from a prior distribution that the model has trained on. This model is a special case where it's prior distribution is comprised of some mix of guassians. think of a multi-model gaussian distribution.

Regression based model
proximity based model


Generative model versus Discriminative:

	Generative models use a prior distribution to find the joint occurrance of a point e.g. the probability of p(y, x) and occurring 

	Where as discriminitive models p(y|x) what's the probability of it being this label given these learned parameters.

	A good question one might ask is: Why is naive bayes better for text classification?


With regard to using standard deviations from the mean
	
	- 'An implicit assumption is that the data is modeled from a normal distribution'. A good rule of thumb would be to measure a Z score > 3 as a proxy for an anomaly.

	operating under the assumption that the 

The first step NO MATTER WHAT in finding outliers in the data is understanding the 'normal' structure of the data first and then picking the right model.
	
	- understanding, which model to use given a certain distribution of data. For example, if there was data which demonstrated a very high linear correlation 2-d or 3-d  linear regression would be the best choice.

Was the way I modeled craiglist the best? 
	- I don't know 

Why non-linear versus linear?
	- I just got better results with linear and you see if put it up on a graph. So within each year it would be normally distributed and having a linear model it wouldn't intersect the tails properly.

I played around with other methods later to intentionally overfit the data but didn't get good results especially with the nonlinear models.

Figuring out a model for outlier analysis is also highly dependant on finding a model that fits well if you find a model that's oversimple and is underfit it will declare something normal as an outlier. If it's overfit than it will overfit to the anomalies.

Though, I feel this is an area that's also up to Analyst's discretion. 

models that make fewer transformations to the data are usually the ones that are the most interpretable. e.g. PCA.

Extreme value analysis?
	Extreme value analysis is actually exactly what it sounds like 

	EVA differs greatly from generative probabalistics models in the context of what's an outlier. EVA is all about the tails and generatives are all about shape of distribution and the probability of that point occurring. Take this set for example {1, 2, 2, 50, 98, 98, 99} where 50 is average and would be considered an anomaly by generative probabalistic model wehere using EVA it wouldn't.

	This distinction gets even more complicated when EVA uses the outputs of a probabilistic model.

	EVA is naturally designed for 1-dimensional data for obvious reasons.

	The bottom line with EVA is that the point(s) must be considered on the outskirts of a dataset in ordered to be considered an outlier.

*Expectation Maximization?
	learning process akin to gradient descent that learns the parameters(weights) of the data. Key distinction about the learning in ML which is ... 

Using a gaussian mixture model for outlier classification?
	assuming the underlying probability distribution has multiple gaussians. It models how well the subject point would fall under each gaussian and the entire distribution as a whole.

figuring out the params through PCA versus normal eqn versus gradient descent?
	ask the author about this piece on page 13. 
	What's the cost function for PCA?

Spectral models?
	Spectral models are used in conjuction with matrix decomposition techniques where the underlying dataset is graph/network based.

Proximity based models
Density based versus clustering based?
	- density segments points and clustering segment the space eg k-means versus db-scan

cluster shapes are assummed

What are the problems with just simply training supervised model of what's an anomaly and not?

	- It's operating under the assumption that you know what an anomaly is it will re-occur in the same form. That is, Fraud can appear in different forms that the model's never seen before.
	- class imbalance
	- multiple kinds of anomalies
	- causal relatonships of anomalies i.e. things occuring in sequence hence they'll be represented by multiple points
	- this method also implies that you've seen anomalies before


How is a kalman-filter used?

Information Theoretic models?
	- it compares the code length i.e. the compression of a information to check whether it's an outlier
	I'm thinking about LSH here but LSH might more sense as a proximity based model sense it hashes similar things into buckets and indexes them s.t. thier local to one another. Jaccard of the minhashes of two things. (this is a very dumbed down explanation, as there are several minhashes taken and compared) 

	example of information theory--
	If you're doing PCA and it's difficult to reduce the dimensions down to something small then that means there isn't that much redundant information hence higher population of outliers w.r.t to the subject data set.

All these learners or models are doing is describing the dataset in some way.
	- clustering descriptions, histograms, plots
	- parametric supervised learning: learning the parameters which describe the data. eg. think about OLS mx+b=y, m & b are the parameters which are learned.
	- PCA, spectral models -- projecting it out to a lower dimensional sub-space or condensed representation of a network.

Proximity based models
---------------------

What are proximity based models?
	- NN
	- DBSCAN
	- K-Means
* Affinity Propagation
* DBSCAN
* K-means
* Adapted kNN

When to use a proximity based model?
Which one to use?



In k-means clustering think about fitting some numbers of clusters to the data. Iterating through the set of data-points computing the silouhette score or some type of entropy score (this is the outlier score) this is outlier score in this case. The one that makes the largest impact is an outlier
	The problem with silhouette score is that it's taken in aggregate, among all points and it's indicative of how well the clusters are formed. We need something that's going to distinguish between clusters, clusters with odd shapes, edge of the data set and inside the dataset.

*You could use ensemble methods in clustering as well?

sequential vs independant ensembles?
	random forest versus GBM

Clustering based techniques--
	- membership of a cluster base
	- silhouette score*
		- the metric which encapsulates how well it fits into a cluster versus how well it doesn't fit into the other clusters
	- How does K-means work?
		- randomly seed centroids
			computes each points distances to each centroid
			assigns each point to it's closest centroid
			the centroid is recomputed (the mean of the points of it's assigned segment)

			this process continues until there isn't movement of the centroid.
KNN--
	- looping through all the points the knn algo willl return indices and distances of it's k nearest neighbor, and now we have all the distances. If there's a repeated pattern of a certian point or a certain group of points being far from eachother, they could be considered outliers.
	O(N^2) best case scenario. Though, this can be more detailed and accurate. It's without a doubt more granular.

How does DBSCAN work under the hood?

Why did I choose a regression based technique versus clustering?
	- I fiddled around with DBSCAN and wasn't getting good results. There would also have to be another process of checking whether or not it was out the ordinary for bad or good reasons.
		- talk about how size of the data is also pretty relevent

Picking the data model given the data and it's size is absolutely crucial. Also think abo

Distinguishing between noise and anomalies:
	non-membership of a cluster just might indicate noise 
	extraordinarily high deviation from center or nearest neighbor might indicate anomaly

What's a good example to distinguish between noise and anomaly in clustering?
	a data point that sits on the fringes of a large cluster is noise
	a data point that sits far away from all other clusters is an anomaly

How can you check the shape of data if it's high-dimensional?
	- tsne might give you some intuition as to how the data sits

WHat is mahalanobis distance?
	- it's a good measure to compute the outlier score.
	- works best with data that's multi-variate normally distributed.
	- Uses PCA to project the data on to a new coordinate space and makes a measurement.
	http://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance

105) go over the EM algorithm

How to combat noise versus anomalies with clustering?
	- don't just checking membership with a cluster technique
	- use distance from a centroid
	- these distances should be normalized e.g. mahalanobis 

	* Clusters should have a minimum threshold on their cardinality in order to be considered true clusters, rather than a closely related group of outliers.

Clustering is used for larger data_sets

If you can get away with it, use k-nearest neighbors
Lookup K-nearest neighbor outlier detection methods
	O(n^2 * d) where d is distance computation
	outlier detect algo knn:
		iterate through set looking for outlier
		once one is found to be outlier throw it in the bag
		if a point is clearly not an outlier do not check that data point again

Cell-based methods--
	- great for lower dimensional data
	- is an alternative to pairwise computation

A good deduping approach would be to work deductively rather than inductively i.e. remove the outliers and keep the big clusters. In the case of dbscann: clusterquality will be to just throw away the smallest clusters and see what's left.

Index-based methods--
	- Look up an R*Tree
		- a data structure used for indexing spatial data. eg. geo-spatial, think about lat lons as inputs
	- need to be good about maintaing a solid inner and outer loop here

Can you compose a distribution of distances with looping through KNN?
	- Perhaps this is a good way to inspect the quality of the clustering

What's reverse KNN--
	basically if you were to think of a graph constructed of the k-nn'd data set. Where ever node is a data point and is given the chance of k edges. If we threshold these edges by some distance metric we'll have outliers with one one out or none at all hence we've identified them.

What is EM? 
	expectation maximization is the statistical techniques of learning (or estimating) model parameters of the underlying data distribution. Solving the MLE for a particular problem is a optimization problem but useing MLE  -- lets say -- to estimate the mean of a normal distribution is a method. Which method of moments could used an alternative.


Advantage of probabilistic models (generative)--
	for example, if that data is categorical then a discrete bernoulli coudl be used for the underlying mixture components.
Cons
	they could fit the wrong distribution type and potentially overfit

If you use a distance threshold that might not work well with distribution that have several sub distribution with varying data sparsity. So a static threshold will identify outliers in one cluster while mistaking things which are noise outliers in the other clusters.


Look this up again: reverse-nearest neighbor approach

What is LOF?
	local outlier factor is a way to quanitify a data points outlierness with adjust for the variation in local densities.

Cons for distance/proximity based search?
	all the points become very far from one another and pratically equidistant. proximity based methods will naturally degrade in quality the higher the dimensions.

Read-up on PCR: ISL - try and understand the motivation behind it

Temporal Data
-------------
- Streaming data: think text.
- Time-Series: an immediate shift in some dimension of the series.
- novelty and change in detection in multidimensional cases
	with sensor data two successive data points are often identical to eachother but in if it's text, this is often not the case in terms of dimensions. 

	Time-instant change: as discussed above. Think of the trend detection bit by twitter or mean-shift, or std deviations out.

	Novelties in individual's data points from one another: len(d_pt1) is largely different from the len(d_pt2). These are often trend-setters as there will be other points to follow and eventually will become a normal data point

	change point outlier: len(d_pt1) is largely different from the average. Leverage's clt. multiple topics change in aggregate

	CONS: these anomalies w.r.t past behavior.

There's a concept of cross-correlation of independant time-series which could also aid in modeling. Think S&P and 10 yr T market

Check out pg 160
	For example, it has been shown in [160] that interesting outliers in widely separated domains such as intrusion detection and text news story analysis are related to the problem of change detection.

Root cause analysis links:
	http://www.datasciencecentral.com/profiles/blogs/great-example-of-root-cause-analysis-to-debunk-the-missile-myth

	http://asq.org/quality-progress/2015/02/back-to-basics/the-art-of-root-cause-analysis.html

What's the problem with high dimensional data?
	It's more difficult to figure out what the normal state of the data is. If you consider the case in proximity models like nn where the higher the dimensions the more the points become equi-distant appearing from one another.


Time-series
-----------
This this is a specific type of temporal data in the sense that there's a time domain and each observed point could be treated as i.i.d in sequential data. So all of those same rules apply

what's a deviation-based outlier?
	one that deviates far from historical figures eg. fitting a regression model and making a prediction and if it's really off from that prediction then it's a large deviation.

	it's good for finding anomalies as well as noise since it's gives you a gradient of how much it was off by i.e. it's easy to produce some outlier score.

Correlations across time:
	aka stream filtering. fitting a model to time-series and seeing how much the next data point deviates from the series.
	should this be rolling or take everything?

Correlation across series:
	eg. two sensors neighboring sensors recording bird calls.

Autoregressive Models
	- regression models that automatically move the modeling window.
	- the errors are the metric for the deviation. they're iid and drawn from a normal distribution. The guassiian properties come in handy with large time-series (p).

Moving Average
	- component to make AR models more robust
	- rolling model that views the deviations from the mean specifically where as AR tries to forecast the upcoming points
	- the error terms here are dependent on the coefficients and implies a non-linear relationship*. So fitting this model cannot be done by least squares

ARMA(p, q)
	- p: the number of values used to fit for the forecasting on the autoregressive component
	- q: the number of terms used for the MA component
	 if p, q are too small than it won't fit the data well and anomalies won't be detected since the threshold for noise will be too high.
	 - double-edged sword; on the one hand, it increases the expressive power of the model, and on the other hand, it could lead to overfitting.

	 note: it's said that whatever time-series you're fitting the model to requires stationarity i.e. mean, variance is constant over this time period.

	 So stationarity is one way of modeling the dependence structure. It turns out that a lot of nice results which holds for independent random variables (law of large numbers, central limit theorem to name a few) hold for stationary random variables (we should strictly say sequences).

	 Many of the asssumptions that hold for iid random variables (law of large numbers, clt, ..) also hold for stationary sequences.

	 if the data can't stationary from transformation or something then we use ARIMA to handle the differencing.

ARIMA
	- is used in a case where a time-series naturally drifts away from the mean, in which case it's common in practice to de-trend the time-series (decompose it) by using ARIMA pg 387 think of a random walk

LAG CORRELATION
Be familiar with the term lag correlation agianst another time series. referring the bird call sensor example there's a constant lag factor associated with the the data each sensor's picking up.

implementation ideas:
	ar model fit
	ma model fit 
	How to properly fit an arma model
	arima
	build an LTSM for time-series and discuss the advantages

Direct generalization of ARIMA, ARMA models:
look for this technique
For example, the Muscles and Selective Muscles techniques proposed in [491] can perform the regression analysis much more efficiently.
Recursive Least Squares is employed to solve the regression more efficiently
 Wold's theorem
 Augmented Dickey–Fuller (ADF)

http://people.stat.sfu.ca/~lockhart/richard/804/06_1/lectures/ModelFitting/web.pdf

How do you use time-series modeling in the context of outlier detection?
	- after properly fitting model s.t. AR, MA, ARIMA, ARMA, or GARCH then seeing how large the error is with a new data point.
	- this could and should be augmented by correct preprocessing procedures like decomposing/detreading or stationarizing etc.

PCA (OR Spectral Model) -- Information compression approach
	The reason why PCA coudl be effective is that i) it captures the correlation in the faeture matrix ii) if you examine the spread of information through the eigenvectors and the first few clearly capture most of the information then this is a good way to construct a supervised model since it's capturing most of the redundancy hence the remaining could be thought of anomalous. If information is spread quite evenly than it means that it's sparse.

	So methods mentioned above highlight stark deviation from expected values and reports them as outliers.

	- failure in sensor equipment or things that are causing volatility in sensor equipment


	supervised approach
	time-series of unusual shapes

	pg236

Cusum -- seems like an interesting approach though the fact that it's seems to needs stationarity first as it's suceptible to FPs from trends.

Approaches to ryan's problem--
	GOAL model a spike in airline prices.
	- it's important to strip out all the outliers before fitting any model to anything, so when you do fit a model you're modeling what is normal
	- ARMAX - well give you the chance to put in seasonal dummies such as, hour, holiday, weekend
	- auto-regressive moving model, keeping track of the errors and seeing where the new point's error falls
	- learning the entire time-series and then looking at the errors
	- much of manuevering will lye on computing the window of time
	- keeping a rolling avg/std and if it's too high then there's a cause for concern, but this only works after decomposition. Static thresholds won't work. Maybe if you leverage CLT it might work out well.
	- supervised approach


Look up:
	- ROOT cause | http://techblog.netflix.com/2015/07/tracking-down-villains-outlier.html
	- What is spectral analysis
	- FFT| http://nerds.airbnb.com/anomaly-detection/
	- FFT | https://www.youtube.com/watch?v=t4zEeujVht8 | https://bugra.github.io/work/notes/2014-03-31/outlier-detection-in-time-series-signals-fft-median-filtering/
	- Trace | https://www.cs.uic.edu/~urbcomp2012/papers/UrbComp2012_Paper18_J.T.Zhang.pdf
	- Supervised learning across multiple time-series
	- Discrete Wavelet Transform (DWT), Fast fourier
	- symbolic Aggregate Approximation (SAX) [305]. In this method, Piecewise Aggregate Ap- proximations (PAA) 
	read 3.4, 3.5, 4, look up that project
	- https://linkurio.us/lyft-vs-uber-visualizing-fraud-patterns/

FFT for outlier detection
-------------------------
https://bugra.github.io/work/notes/2014-03-31/outlier-detection-in-time-series-signals-fft-median-filtering/

What is a transformation?
	it transforms or maps data from one domain to another. Example non-linear to linear or time to frequency. For example PCA and the normal equation are examples of linear transformations. If you really wanted to be anal you could call it an orthogonal linear transformation.

	Gets you from one representation of the data to the other. We could also think of this occurring in a conv net as an image passes through conv layers, it's filtering out the image to specific features.

What are Fourier transforms?
	they literally transform data from the time domain to the frequency domain represented by sinusoids. sine waves were the best becuase they're just easier to model as they don't change shape just amplitude and phase. We could easily monitor change in phase for our purposes of anomaly detection.

What are some low-tech ways of finding outliers?
	Rolling Median average as opposed to mean averaging. Why? because median is robust to outliers and it doesn't smooth the original signal though it could be quite jumpy with high/low peaks. In the context of identifying outliers it works quite well. Though might not be the best at recreating the normal state of the model.

Why is it dangerous to use a global threshold in rolling models?
	global thresholds operate under the assumption the normal state of the data is consistent all throughout the stream. Something more dynamic and specific to it's window of data might make more sense.

what is cascading?
	sequentially layering of models.

What are evaluation methods?
	recreate the original signal with whatever modeling technique and you can grid search the optimal parameters e.g. window size

FFT versus rolling median?
	FFT, seems to work very well. I'll have to think more about it's weakness when it comes to anomalies detection. It seems to have a problem with noise but anomalies it doesn't.

What's the problem with rolling and/or windowing methods?
	trying to figure out the size of the window and if the window's too small it'll miss some anomalies not in the context of single points but a sequence of points or a window of points e.g. the flash crash.

Quick and dirty of how to deal with outliers?
	- leave it as is and include it into the training set
	- eliminate it
	- winsorize it
		Basically either assign it a lesser wieght or modify it making it appear close to the rest of the data points.