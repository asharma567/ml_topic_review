- canary 

- How does productionalizing work? (step by step) from prototyping to productionalizing?
	- test environment
	- canary deployments
		- small subset of users 5%, using something like optimizely
		- research what is a endpoint?
			- round robin load balancer

	- pickle it and version it
	- testing for extremes
		- passing a data type
		- overfit or underfit

	"doing data science" armadillo


i) The step by step of deploying a model from prototype to production (best practices). Load balancing, tests, or anything else taken into consideration.

ii) What are key distinctions of data scientists who work in the production code-base? how are they evaluated? how is the interview process different versus for regular data scientists.

It's common thought that data scientists prototype a model an send it off to a data engineer. I'm looking to bridge the gap in between and I think it's a very valuable skill perhaps it might make for a good talk or blog.

What are some common misconceptions Jr data scientists have about prototyping and deploying code.

what edge cases are you testing for?
how are you automating models?

I have yet to come across good resources or material do you recommend?
Any popular tools?




Add REST endpoints:
	Post, to post your regression data rows.
	Get, to get proper score back for a certain factor or attribute.
	Update previous added datasets.
	and lastly Delete, for removing noise/or not needed data.

What is NGINX plus?
	It's a prepackaged load balancer?

What's round robin balancer?
	It's a load balancing paradigm where it distributes the requests to each node uniformly. One by one it simple rotates and the resulting buckets get filled uniformly.

what is PEP 333?

what is CerryPy?

what is Gunicorn?

what is Tornado?

what is mod python?

Put versus post?

sklearn non-parametric

health metrics for prediction, and training. cabana logs, graphana metrics

testing models before rolling predictions, figuring out good edge cases, testing for preprocessing. Testing each stage of the way.

all the basic points: 
- cracking the coding interview
- knowing the data-structures, understanding time complexity trade-offs datastructures.
	- TRIE or Doubly Linked-lists would be a bit mucho

- unit testing
- version control
- writing modular code Dont repeat yourself

Pickling doesn't necessarily work well

He likes saving the parameters to a table and scoring observations through the points because it's a light weight engineering solution that's fail proof

using the other method can have issues like version controlling and dependency issues.

======================
Lookups
	Also look up spinning virtual environements
	the trade-off between rabbitmq and kafka
	different endpoints for a flask app
	loadbalancing

come up with specific questions to ask this guy:
	https://www.quora.com/What-is-the-easiest-way-to-deploy-a-machine-learning-model-say-a-regression-for-production

Come up with unit tests for each stage of the ML pipeline specifically for a/b testing a new model against an old one. eg canary deployments

What's the distinction between Kafka and RabbitMQ?

What are the trade-offs between option 1 and 2 for model deployment

	1) assumes a parametric model but much less engineering over head
		eg. OLS, train beta weights in R or something, and use SQL to score.
	2) serialization, version controlling issues and dependency issues.

Think about how we did this with Zoltar.


How did you deploy a model in Ten-x?
-------------------------------------
- the defacto standard 
	storing the parameters on a table and doing the computation through pig/hive scripts.automating it using oozie workflow.
	
	PROS-light weight design, easy to update weights, no dependency issues
	CONS-only parametric models

- api approach
	use a flask app to wrap a serialized model in a RESTful service. 
	Make different endpoints for it. 
	Use AWS load balancer 
	NGINX
	Create unit tests for every step of the ML pipeline: preprocessing, train, predict

	PRO-Non-parametric, easy protoype to productions
	CON-version control, dependency eg prototyped using a different model

best practices
deployment
	- health metrics for prediction, and training. cabana logs, graphana metrics
	- canary deployments


Productionalizing models
=======================
- Can you walk me through how you go from prototyping to productionizing a model or any change you've made to a pre-existing one step by step?
- How do you A/B a model and/or change? Which test do you use? Which metrics do you look for? How you are you sampling? What are the assumptions?
- What else do you check before productionizing a model? what are some of the edge cases you think about?
- Does anyone review your code?
- How do you handle load balancing issues?
- What do you have to know about data engineering?


What did I learn from Zocdoc?
-----------------------------

What's the differences between a relational data base and nosql data bases? What are the trade-offs?

How do you optimize search?

Had I spent about an hour hypothesizing problems before the interview I would've done better. The idea was to get a better sense of their product: match-making between patients and doctors and how one would go about solving that.

If you were building out a feature matrix. How would you solve for conversions of patients and doctors. What information do we collect about the doctor and what information do we know about the patient?

*I didn't know what information they had about users

How does Kafka stand between everything? what real-time stores am I using?

simple randomization of users when rolling out a model to a very small segment of users agnostic to other variables.

How else would you scale this problem s.t. it's minimize computation

What is Kafka, flume, solace

What is hdfs, s3, 

What is Redis?

What is Nginx

what's spark streaming?

spark versus storm?

	spark stream does computation in micro-batch whereas storm does one at a time
	Storm has a micro-batch api called trident

	Spark is a batch processing framework and as an accessory does micro-batching
	Storm is a stream processing framework and as an accessory does micro-batching

Spark has some issues with being fault tolerant

What is storm?
	core storm
		one at a time
		low latency
		operates of tuple streams

		langs
			everything
			python
			Scala
			Java



	trident
		micro batch
		higher throughput
		operates on Streams of tuple batches and partition

		langs
			Java
			Clojure
			Scala
			*python only available on Spark streaming

	what is fault tolerance?

	examples of production deployments--
		http://storm.apache.org/releases/current/Powered-By.html
		http://engineering.sharethrough.com/blog/2014/06/27/sharethrough-at-spark-summit-2014-spark-streaming-for-realtime-auctions/

how is storm used in a production setting

great source for how to handle streaming events?
	http://www.slideshare.net/ptgoetz/apache-storm-vs-spark-streaming

what is flink?
what is lambda architecture?

KAFKA
JMS 
JDBC (hbase)
what's a pojo
What is Kinesis