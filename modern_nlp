why DistilBertModel vs anything else?
	DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased , runs 60% faster while preserving over 95% of Bert's performances as measured on the GLUE language understanding benchmark.
- what is hierarchical Bayesian model?
- how do we get around the memory allocation issues?
	GCP to allocate more mem

- is there a way to make an 4+ hour autoshut off?
- make a tier is much cheaper

- what's an autoencoder do?


what is encoder-decoder?
	
	One of the benefits of deeplearning is that it does feature engineering for you. Much more so, than tree based models. Encoding raw data to a feature space (mapping) is very much what encoding is. 

	Like raw text to embeddings and then using those embeddings to form a cluster or passing it to a classifier.

	however, if you need an output like a reply to a sentence, or different representation of an image. then you'll need to use a decoder. "machine-translation" is a good word to capture this.

	src: https://www.quora.com/What-is-an-Encoder-Decoder-in-Deep-Learning


What is attention?
	feeble understanding, but it's regarded as the most powerful concepts in deep learning.

	in machine translation (see above) the longer the sentence the easier it was to lose context of the words at the beginning, attention resolves this.

	Basically, when generating the ouput word, how much should we paying attention to each of the words in the input in sentence. Or the attention weight for the corresponding word and it's position e.g. if we're doing a languange translation.

	https://www.youtube.com/watch?v=quoGRI-1l0A

	https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f

what's an attention mask?
	it's used when batches have varying lengths of sentences.

What's a transformer?
	
	TLDR

	RNNs were sequence based which makes it slow to process word embeddings, vectorizations like word2vec weren't rich enough. So transformers were made were made. It would process word embeddings simultaneously and leverages GPUs much better than the it's predecessors.

	Notice how much more complex the NLP models are in comparison to computer vision models. 

	That's probably because it's the most sophisticated function of a human, the thing that distingiushse our species vs others: Communication.

	The evolution:

	RNN's were initial used to model langauge because of it's implicit sequential nature or markov chain. 

	But it had problems with log sequences: exploding and vanishing gradients and it was slow to train. 

	LSTM to answer that. It also gave it the net bi directionality but it was still sequential in nature (ergo sequence to sequence modeling).

	Tfidf and word2vec also weren't good "language mdoels" meaning the word embeddings weren't rich enough to capture the semantic meanings&relationships.


	what's an embedding space?

		words post vectorization get mapped to an embedding space and the distances between the word embeddnigs can give an idea of their relationship to one another.

		If the position of the word within a sentence is also used, we get context + word vector = richer word embedding 



	What's the best model out to date (May 2020)
	- XLNET is the best transformer to date. Supposed to be better than BERT

Word2vec vs Bert?
	The problem with word2vec is that each word has only one vector but in the real world each word has different meaning depending on the context and sometimes the meaning can be totally different (for example, bank as a financial institute vs bank of the river)

What are word embeddings?
	vector representation of words. Modern methods include the context as well (relative position)


re: NLP, what can we do now that we couldn't do before use-case wise?

What is a learning rate scheduler?
	depending one varies the learning rate, you'll have difference in loss total lose and rate of convergrence. 

Why is a warm-up necessary?
	If your data set is highly differentiated, you can suffer from a sort of "early over-fitting". 

	If your shuffled data happens to include a cluster of related, strongly-featured observations, your model's initial training can skew badly toward those features -- or worse, toward incidental features that aren't truly related to the topic at all.

	Warm-up is a way to reduce the primacy effect of the early training examples. Without it, you may need to run a few extra epochs to get the convergence desired, as the model un-trains those early superstitions.

	https://stackoverflow.com/questions/55933867/what-does-learning-rate-warm-up-mean


How to combat exploding gradient?
	- gradient clip hack


what is eval_mode?
	it's just like the training mode but regularization (dropout) and batch normalization aren't enabled.

	that's why you'd want to state : no gradeint #no modifications to the training weights.

NLP had a pivotal moment in 2018 where - like in CV - discovered the power of transfer learning. This is post word2vec and glove. As such, the model is passed on as well i.e. we no longer have to train NLP models from scratch.

