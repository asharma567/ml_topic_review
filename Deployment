# Deployment: 


### Motivation
For a couple of years, I researched best-practices for model deployment at the time there wasn't much available as such I've made this post to share my learnings from my recent journey: __A data scientist operating with minimal support, no data/ml engineers or devops team.__ 

As you could imagine, there's a decent amount of interaction with engineers during this phase so I wrote this to support arguments for my methodology from the perspective of cost-benefit tradeoffs.

### Distinctions in Architecture Paradigms:

Soa over monolithic
-------------------

SOA:
+ isolation: 
	Prevents dependency and versioning issues. ML package versioning is a big problem in that it evolves pretty quickly and would cause problems in a monolithic structure. E.g. pandas could make a some major changes to dataframes from one version to another e.g. changes/updates in pandas used in model_for_fraud_detection will have an impact on model_monitoring_churn.

+ reduces engineer time/involvment:
	scenarios which involve the model being updated:
		* tuning) DS
		* retraining ) DS
		* feature engineering NOT involving addition data inputs) DS
		* feature engineering involving addition data inputs ) DEV
		* model deteriotion
		* data deprecation) DEV
		* sudden failure in performance) Devops -> DS, may require a dev in 1

+ root cause analysis:
	Easier to figure who owns the service and which engineers to contact should something break.

+ scalable:
	Could have one service interface with many clients though you could also do this if you invoke the function locally.

+ ownership and authors:
	Easier to find the owners and authors of the api.

+ error handling
	Should be delivered in the client response and should be done at the api level so that it's scalable across requests anostic to client.

SOA vs microservices
<<<<<<< HEAD
--------------------
This is the subtle distinction of wrapping the entire dataproduct within a service i.e. preprocessing, prediction, and response (SOA) or decompose them to smaller services which call eachother (microservices).
=======
------------------------
wrapping the entire dataproduct within a service i.e. preprocessing, prediction, response. This is a good first step towards a true microservices archtecture where even the former: preprocessing & prediction are invoked independently.
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d

Much like best-practices used in software development, good design is flexible to change and reusable. Analagously the motivation for microservices e.g. two seperate models which use the same feature matrix. they would each call the same preprocesing service.

Though we went with SOA, I think that's an initial step to mircoservices.


## Tools for hosting the service

Lambda vs EC2+ECS
-----------------
Lambda
------
+ devops light and don't have to worry about performance monitoring
+ scales automatically
+ server is self managed
- it has limitations: 50mb, 5min timeout, cold start, 2.7 standard libraries, 3.6 incomplete
	+ size can be combated by decomposition of functions. Lambda is great-way to a true microservice architecture.
- can't ssh into server, AWS Lambda does not provide any control infrastructure & environment used to run the application code
+ takes a zip file restrained by specific AMI environment provisions.


ECS
---
+ takes a docker image so it allows for more flexibility: language, size, etc
- better optionality for setting-up infrastructure. since this an area that's well outside my profession, it only serves as a distraction.
- involves managing a server, scaling, etc.
- this would also use flask frame-work which - for me - has been unreliable.

Given you've already paid the learning pains, lambda makes much more sense for a data scientist who's looking for a hands-off approach.




Making deployment zip for lambda
-------------
This is the final step before uploading to lambda.

As consequence to the provisioning benefits of using lambda, we need to build software such that it's compatible with it's environment and size restrictions. Here's how to do it--

_*if zipped deployment package is less than 50mb, skip to (7)_

1. install docker
2. docker pull down the same AMI lambda's using
```
docker pull amazonlinux:2016.09
```
<<<<<<< HEAD
	#!/bin/bash
	set -ex

	yum update -y

	yum install -y \
	    atlas-devel \
	    atlas-sse3-devel \
	    blas-devel \
	    gcc \
	    gcc-c++ \
	    lapack-devel \
	    python27-devel \
	    python27-virtualenv \
	    findutils \
	    sqlite-devel \
	    zip

	do_pip () {
	    pip install --upgrade pip wheel
	    pip install --use-wheel --no-binary numpy numpy
	    pip install --use-wheel --no-binary scipy scipy
	    pip install --use-wheel sklearn
	    pip install --use-wheel joblib && \
	    pip install --use-wheel pandas && \
	    pip install --use-wheel py-stringmatching && \
	    pip install --use-wheel pytz && \
	    pip install --use-wheel unidecode && \
	    pip install --use-wheel nltk && \
	    python -m nltk.downloader stopwords

	}

	strip_virtualenv () {
	    echo "venv original size $(du -sh $VIRTUAL_ENV | cut -f1)"
	    find $VIRTUAL_ENV/lib64/python2.7/site-packages/ -name "*.so" | xargs strip
	    echo "venv stripped size $(du -sh $VIRTUAL_ENV | cut -f1)"

	    pushd $VIRTUAL_ENV/lib64/python2.7/site-packages/ && zip -r -9 -q /outputs/venv.zip * ; popd
	    echo "site-packages compressed size $(du -sh /outputs/venv.zip | cut -f1)"

	    pushd $VIRTUAL_ENV && zip -r -q /outputs/full-venv.zip * ; popd
	    echo "venv compressed size $(du -sh /outputs/full-venv.zip | cut -f1)"
	}

	shared_libs () {
	    libdir="$VIRTUAL_ENV/lib64/python2.7/site-packages/lib/"
	    mkdir -p $VIRTUAL_ENV/lib64/python2.7/site-packages/lib || true
	    cp /usr/lib64/atlas/* $libdir
	    cp /usr/lib64/libquadmath.so.0 $libdir
	    cp /usr/lib64/libgfortran.so.3 $libdir
	}

	main () {
	    /usr/bin/virtualenv \
	        --python /usr/bin/python /sklearn_build \
	        --always-copy \
	        --no-site-packages
	    source /sklearn_build/bin/activate

	    do_pip

	    shared_libs

	    strip_virtualenv
	}
	main
=======
3. build in lambda the environment using lambda (takes time)
```
docker run -v $(pwd):/outputs -it amazonlinux:2016.09 \
/bin/bash /outputs/build_dependencies_in_lambda_environment.sh
```
4. it'll output all the dependencies compressed as binaries in a zip called: 'venv.zip'
5. unzip this and add all other scripts: lambda_handler, main function,..
6. trim weight down to 50mb (zipped)
7. test full deployment package with lambdaci's docker image 

before you do this step, i) install lambci ii) rename script containing the handler to 'lambda_function.py' and the handler function to 'lambda_handler'
```
docker run -v "$PWD":/var/task lambci/lambda:python2.7
```

other techniques on sizing down:
1. go into the lib folder and remove everything with an '.a' extension
2. from venv delete the docs e.g. pandas-0.20.2.dist-info and pip and anything you think you don't need
3. strip all .pyc files as they aren't needed

>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d

build_dependencies_in_lambda_environment.sh
```
#!/bin/bash
set -ex

yum update -y

yum install -y \
    atlas-devel \
    atlas-sse3-devel \
    blas-devel \
    gcc \
    gcc-c++ \
    lapack-devel \
    python27-devel \
    python27-virtualenv \
    findutils \
    sqlite-devel \
    zip

do_pip () {
    pip install --upgrade pip wheel
    pip install --use-wheel --no-binary numpy numpy
    pip install --use-wheel --no-binary scipy scipy
    pip install --use-wheel sklearn
    pip install --use-wheel joblib && \
    pip install --use-wheel pandas && \
    pip install --use-wheel py-stringmatching && \
    pip install --use-wheel pytz && \
    pip install --use-wheel unidecode && \
    pip install --use-wheel nltk && \
    python -m nltk.downloader stopwords

}

strip_virtualenv () {
    echo "venv original size $(du -sh $VIRTUAL_ENV | cut -f1)"
    find $VIRTUAL_ENV/lib64/python2.7/site-packages/ -name "*.so" | xargs strip
    echo "venv stripped size $(du -sh $VIRTUAL_ENV | cut -f1)"

    pushd $VIRTUAL_ENV/lib64/python2.7/site-packages/ && zip -r -9 -q /outputs/venv.zip * ; popd
    echo "site-packages compressed size $(du -sh /outputs/venv.zip | cut -f1)"

    pushd $VIRTUAL_ENV && zip -r -q /outputs/full-venv.zip * ; popd
    echo "venv compressed size $(du -sh /outputs/full-venv.zip | cut -f1)"
}

shared_libs () {
    libdir="$VIRTUAL_ENV/lib64/python2.7/site-packages/lib/"
    mkdir -p $VIRTUAL_ENV/lib64/python2.7/site-packages/lib || true
    cp /usr/lib64/atlas/* $libdir
    cp /usr/lib64/libquadmath.so.0 $libdir
    cp /usr/lib64/libgfortran.so.3 $libdir
}

main () {
    /usr/bin/virtualenv \
        --python /usr/bin/python /sklearn_build \
        --always-copy \
        --no-site-packages
    source /sklearn_build/bin/activate

    do_pip

    shared_libs

    strip_virtualenv
}
main
```

<<<<<<< HEAD
techniques on otherways to get the file size down:
	1. first just try and zip and see if that's good enough
	2. if that doesn't work try the script above to compress the libraries into binaries
	3. go into the lib folder and remove everything with an '.a' extention
	4. from venv delete the docs e.g. pandas-0.20.2.dist-info and pip and anything you think you don't need
	5. all .pyc files, aren't needed

Main goal is to do away with stuff you don’t need within a library. As simple as this sounds it’s pretty difficult given time-contraints eg. NLTK’s a very interesting package as it requires more than the simple pypi package, there’s an entire library of corpora. To avoid installing everything I shaved it down to a bear minimum of stopwords-english and wordnet while maintaining the original directory structure (found in nltk_data).

However, note the change in call path—
	```
	import nltk
	nltk.data.path.append("/var/task/nltk_data")
	from nltk.corpus import stopwords
	from nltk.stem.snowball import SnowballStemmer
	from nltk.stem.wordnet import WordNetLemmatizer
	```
=======

NLTK requires more than the simple pypi package, there’s an entire library of corpora. To avoid installing everything I shaved it down to a bear minimum of stopwords-english and wordnet directories while maintaining the original directory structure (found in 'nltk_data').

*note the change in call path
```
import nltk
nltk.data.path.append("/var/task/nltk_data")
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
```

Basically zipping up the entire contents of the directory. **It's very important to note this distinction: zipping up the contents of a folder vs zipping up the folder.**
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d

build_deployment_zip.sh
```
#!/bin/bash

<<<<<<< HEAD
Zipping
-------
Zip up the entire contents of the directory. It's very important to note this distinction: zipping up the contents of a folder vs zipping up the folder.

zip.sh
	```
	#!/bin/bash

	echo 'Creating deployment zip...'
=======
echo 'Creating deployment zip...'
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d

git archive -o deploy.zip HEAD

echo 'Done.'
```

Compressing the libraries
-------------------------
Prior to this, there's also a process to the compress the libraries of model itself. One provision of Lambda is a 50mb limitation of the zip - _large pain point for me._

<<<<<<< HEAD
__Special considerations:__
	- lamdbda cold start, 5 min timeout. If the service doesn't get a heartbeat in 5 mins it'll aws will kill the service. 
	- 2.7 standard libraries, 3.6 incomplete. As of this writing, you'll likely have downgrade your app in python 2.7. This sounds like a bigger deal then it actually is, many engineers oppose this but the change is trivial from a time perspective.
=======
keep in mind w lambda:
	- lamdbda cold start, the first call will be slower than normal
	- 2.7 standard libraries are full and 3.6 are incomplete which is why we downgraded to 2.7 during this particular run.
	- 5 min timeout. It'll kill your function if it takes longer than 5 mins to run.
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d


Modify lambda handler with--

lamnda_handler.py
```
import ctypes
import json
import os

from flattened_data_points import FlattenedDataPoints
from predict import score_all_data_points


#this portion of the code decompresses the libraries
for d, _, files in os.walk('lib'):
    for f in files:
        if f.endswith('.a'):
            continue

        ctypes.cdll.LoadLibrary(os.path.join(d, f))
        print("success", str(d), str(f))

def handler(event, context):
    print('Start matching expenses to budgets')
    print('Received the following input:\n{}'.format(json.dumps(event)))

    flattened_data_points = FlattenedDataPoints(event).all()
    scores = score_all_data_points(flattened_data_points)

    print('Scores:\n{}'.format(json.dumps(scores)))
    print('Finished')
    return scores
```


resource:
	https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/




<<<<<<< HEAD
Making the end-point on aws
------------------
resources: https://www.youtube.com/watch?v=8U4RRw3PwGw
if I had to redo this part I'd use: https://serverless.com/framework/docs/providers/aws/guide/intro/

2) steps to create the lambda function:
	
	- login to console and create the lambda function:
		https://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#/functions?display=list
	- select runtime environment as python 2.7 and select blank function
	- add api gateway
	- deployment stage: test (end point we already made)
	- security: open
	- name the function 
	- upload your code
	- existing role

	- Making an endpoint (api gateway)
		- Services->api gateway
=======
Making the end-point
--------------------

If I had to redo this I'd use: https://serverless.com/framework/docs/providers/aws/guide/intro/

1) create the lambda function:
	
	a) create the lambda function on the console
	configurations:
		 - https://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#/functions?display=list
		 - select runtime environment as python 2.7 and select blank function
		 - add api gateway
		 - deployment stage: test (end point I already made)
		 - security: open
		 - name the function 
		 - upload your code
		 - existing role

	b) making an endpoint (api gateway)
		- services->api gateway
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d
		- Actions->create resource
			- Create resource
			- Create method (this is the endpokint)


<<<<<<< HEAD
	# write lambda_function.py	
=======
	c) write main test function: 'lambda_function.py'	
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d
	```
	import json

	print('Loading function')


	def lambda_handler(event, context):
	    print("Received context: " + str(context))
	    print("Received event: " + json.dumps(event, indent=2))
	    
	    return json.dumps(event, indent=2)
	```

<<<<<<< HEAD
	- name: lambda_function.lambda_handler
	- create a trigger with an api gateway

3) create the api gateway/post endpoint to the lambda function
	
	_*it's key to understand there's a seperate endpoint that needs to be created to expose the function publicly as opposed to within the aws infrastructure_

	### CREATE METHOD
	- create a GET and POST seperately
	- make sure the region you select for the api gateway is the same as the lambda function
	- do not check lambda proxy configuration
	- integration type lambda function

	### CREATE stage
	- grab the api key from here

	
	Exposing the API externally and invoking lambda function
=======
	- name of this function: lambda_function.lambda_handler
	
	d) create trigger with an api gateway

2) create the api gateway/post endpoint to the lambda function
	
	**There's a seperate endpoint that needs to be created to expose the function publicly as opposed to within the aws infrastructure.**

	a) CREATE METHOD
	- create a GET and POST seperately
	- make sure the region you select for the api gateway is the same as the lambda function
	- do not check lambda proxy configuration
	- integration type lambda function

	b) CREATE stage
	- grab the api key from here

	c) Exposing the API externally and invoke lambda function
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d
	```
	{"message": "Internal server error"}'
	```
	if you're getting this error there's likely a problem with the endpoint configuration.

<<<<<<< HEAD
	create Resource
	create POST Method
		Integration type; Lambda Function
		DO NOT CHECK LAMBDA PROXY INTEGRATION
		Lambda Region us-west-2
	after it's created, test it internally by copy-pasting the data into request body

	be sure to "Deploy API" though this trivial from a configuring perspective, it needs to be done

	#handler used to troubleshoot
	```
	def lambda_handler(event, context):
	    raise Exception('the sky is falling!')
	```
=======
	- create Resource
	- create POST Method
		- Integration type; Lambda Function
		- DO NOT CHECK LAMBDA PROXY INTEGRATION
		- Lambda Region us-west-2
	
	after it's created and test it internally by pasting the data into request body

	be sure to click "Deploy API" though this trivial from a configuring perspective, it needs to be done.	
>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d

	reference:
	http://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started.html#getting-started-new-post

<<<<<<< HEAD
4) 
	Versioning and fault tolerance
	====
	- versioning, How to do it through cli (http://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html) though there's an even easier approach through the console. Alternatively, could also have the function call the deployment package from an s3 bucket and could just change the pointer.
	- fault tolerance, you can put up mirrors in various regions incase one goes down
	- security, allows for auth with new credentials.
=======
3) How to handle versioning and fault tolerance
	====
	- lambda functions are abled to be versioned pretty easily
	- fault tolerance, you can put up mirrors in multiple regions incase one goes down for security. Always online and not subject to changes due to upgrades.
	- also allows for auth with new credential for security reasons

>>>>>>> ffc9d161d5e6dd1d80d62c138259ddafd868992d


FAQs
-----


### Should you wrap the function as a docker image and send away to productionize it?

	Sure but keep everything together should anyone need to make minor adjustments and rebuild, we used a single git repo solely for this. In our case, both an engineer & data scientist own the api.

### The repo
	The production level code should live in a seperate repo from all the code that lives in general data science folders (sandbox). 
	i) if there's something an engineer can fix e.g. handling nulls in the preprocessing chain, they could do it themselves and rebuild the image within the repo 
	ii) this holds even if a data scientist has to fix it as well.

	Having everything contained in one place is nice for--
		i) rapid adjustments 
		ii) discoverability 
		iii) subsequent iterations 
		iv) general information about the service within documentation: README.md

	README.md
	--------
	- name of function and version
	- business function/question the product was built for
	- metrics of interest: business & data science
	- how monitor performance e.g. corresponding datadog dashboard or sql queries to monitor performance
	- instructions on how to build the docker image which is sent up the model:
		```
		Deployment

		To create the deployment package, run the following command while in the root directory of the project:

		$ ./build

		This creates a new file called deploy.zip in the current directory, which can then be uploaded to Amazon Lambda.
		```
	- testing locally before sending it up to lambda
		```
		Executing Locally

		While in the root directory of the project with Docker running, run the command:

		$ docker run -v "$PWD":/var/task lambci/lambda:python2.7 main.handler JSON_PAYLOAD
		Replace JSON_PAYLOAD with a JSON formatted payload sent as a string. For example:

		$ docker run -v "$PWD":/var/task lambci/lambda:python2.7 main.handler '{"budgets": [{"budget_price_budget": "140.0", "travel_vendor_receipts": "Southwest Airlines", "purchase_vendor_receipts": "Southwest Airlines", "budget_id": "309366", "start_datetime_trips": "2017-04-18 04:00:00", "generated_at_budget": "2017-04-08 14:55:39", "budget_type_budget": "flight", "actual_cost_budget": "131.98", "end_datetime_trips": "2017-04-19 04:00:00"}], "all_user_expense_lineitems": [{"expensed_amount_itemization": "24.0", "expense_type_name_itemization": "Taxi", "transaction_date_itemization": "2017-04-25 00:00:00", "itemization_id": "4659496", "expense_category_itemization": "taxi", "vendor_name_expense": "taxi", "expense_type_name_expense": "Taxi", "expense_category_expense": "taxi"}]}'
		```

	- testing at api levl
		```
		Alternatively, you can use a tool like Postman to send your JSON payload to the following address:

		https://j4e8yp6cdi.execute-api.us-west-2.amazonaws.com/test/budget_expense_matcher/
		```

	- Request Format
		```
		Request body:

		{
		    "budgets": [
		        {
		            "budget_price_budget": "140.0",
		            "travel_vendor_receipts": "Southwest Airlines",
		            "purchase_vendor_receipts": "Southwest Airlines",
		            "budget_id": "309366",
		            "budget_type_budget": "flight",
		            "generated_at_budget": "2017-04-08 14:55:39",
		            "start_datetime_trips": "2017-04-18 04:00:00",
		            "actual_cost_budget": "131.98",
		            "end_datetime_trips": "2017-04-19 04:00:00"
		        }
		    ],
		    "all_user_expense_lineitems": [
		        {
		            "expensed_amount_itemization": "24.0",
		            "expense_type_name_itemization": "Taxi",
		            "transaction_date_itemization": "2017-04-25 00:00:00",
		            "itemization_id": "4659496",
		            "expense_category_itemization": "taxi",
		            "vendor_name_expense": "taxi",
		            "expense_type_name_expense": "Taxi",
		            "expense_category_expense": "taxi"
		        }
		    ]
		}
		The response should look like the following:

		[
		    {
		        "score": 0,
		        "budget_id": "309366",
		        "itemization_id": "4659496"
		    }
		]
		```
Example: Calling out using boto3
--------------------------------

Assuming the request body looks like the one above...


The following function would be used to call out to the api.

```
import boto3
import simplejson
import settings


def get_model_scores():
    client = boto3.client('lambda', region_name=settings.LAMBDA_REGION_NAME)
    payload = get_payload()
    response = client.invoke(
        FunctionName=settings.LAMBDA_FUNCTION_NAME,
        InvocationType='RequestResponse',
        Payload=bytes(simplejson.dumps(payload), 'utf8')
    )

    decoded_response_payload = simplejson.loads(response['Payload'].read().decode())
    
    return decoded_response_payload
```


Lessons learned from designing the API:
    - Error handling should be done at the api level not at the client level -- suppose things aren't broken down to a very granular microservices level -- the service should complete it's job rather than error out and fail on the whole job.

    Lambda functions have a call cost associated PER call so we thought to combat this we'd send up a larger payload and have all the computation done at the api level. The api was made very brittle such that it would break if there was a null in any of the fields. This was causing a compounding effect in terms of the business flow it was suppose to augment. 

    Solution:
    i) using try/catch blocks wrap the errors in the reponse back to the client

    ii) Break the calls into smaller chunks and log the input outputs to api



