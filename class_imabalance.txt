How do you deal with class imbalance?
-------------------------------------
Model adjustments
- example reweighting-
	- Bayesian classifiers can deal with it by adjusting weights to the prior and conditional probabilities.
	- Decision Trees split criterion for classes is often proportional to the class distribution. Gini impurity index or entropy.
	- instance based (KNN) votes can be weighted.
	- A symmetrical cost-functions: https://cran.r-project.org/web/packages/expectreg/index.html

- One class learners
	- though this is fit once you have a dataset pruned of the minority class and widely used for novelty detection i.e. the minority class isn't consistent. This would be a outlier removal technique though, it would misclassify anomalous behavior that is. It might not do a great job of distinguishing between noise and anomalies.

- Grid-searching hyper-parameters
	- you can optimize the specifity or sensitivity by grid-searching hyper parameters precision and/or recall

- Cut-off points
	- there are obviously various cutoff points which. how do you find the point at which it's optimal for both specitity and sensitivity?

Data adjustments
- sampling 
	OVER: SMOTE and Noisy PCA.
		smote -- http://www3.nd.edu/~dial/publications/chawla2005data.pdf

		noisy pca -- http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6033567&tag=1&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6033567%26tag%3D1

	UNDER: Hacking away at your majority class until your training set is balanced. Though there's an obvsious loss of information using this method. so it woudl be prudent use something more complex like ensembling. 

	eg. save each portion of the hacked away class and copy-paste the minority class to each one, use ensembling to figure out the votes.


=====
